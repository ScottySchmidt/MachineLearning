{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit Risk Mortgage Loans\nThe data is provided by [Home Credit](https://www.homecredit.net/about-us.asp), who provides lines of credit (loans) to the unbanked population. There are 307,511 rows of different credit information and 122 columns of feature variables. \n\nUsing different models using AUC found the following:\n* Linear regression 0.7327\n* Decision tree 0.7215\n* Gradient boost 0.6966\n* Random forest 0.6312\n* Logistic regression 0.6145\n* Knn 0.5106 \n* SVM has runtime memory issues from too big data. \n\nA final credit prediction will be made for each output.\n\n## Datasets Summary\nOriginal dataset csv files can be found on [Kaggle](https://www.kaggle.com/c/home-credit-default-risk). The columns with first five rows will be shown below to view whenever a dataset is used. Therefore, one will not have to download the csv files. There are seven sources of data for this project which will be briefly\ndescribed below:\n* Train.csv: This is the most important dataset with 307,511 rows which are house data. There are 106 column features describing houses such as square feet and year built. The column TARGET column is an important feature to discuss. A 1 in this row means the loan struggled to payback. A 0 means the loan was did not default. Some of the features will need to be encoded numerical to test if they have high feature importance.\n* bureau.csv: Other previous credit data from other financial institutions. \n* bureau_balance.csv: Monthly bureau previous credits.\n* brevious_application.csv: Previous application loans.\n* POS_CASH_BALANCE.csv: Monthly data about previous cash loans. \n* credit_card_balance.csv: Monthly credit card data for clients with Home Credit.\n* installments_payment.csv: Payment history for previous loans.\n<br/> <br/>\n\n## View Train Data\nThe training dataset is the most important dataset with over three-hundred thousand house prices that will be predicted at the very using the best metrics predictive models with reduced error. In total, the train data has 122 columns with 307,511 rows of credit. The first five rows of the train.csv file will be shown below.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom statistics import mean\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.datasets import make_classification\nfrom sklearn import ensemble\nimport sklearn.metrics as metrics\n#sample=r'/kaggle/input/home-credit-default-risk/sample_submission.csv'\n#cash=r'/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv'\n#info='/kaggle/input/home-credit-default-risk/HomeCredit_columns_description.csv'\n#app=r'/kaggle/input/home-credit-default-risk/previous_application.csv'\n#cc=r'/kaggle/input/home-credit-default-risk/credit_card_balance.csv'\n#install=r'/kaggle/input/home-credit-default-risk/installments_payments.csv'\nbureau_balance=r'/kaggle/input/home-credit-default-risk/bureau_balance.csv'\ntrain=r'/kaggle/input/home-credit-default-risk/application_train.csv'\ntest=r'/kaggle/input/home-credit-default-risk/application_test.csv'\n\n#PERSONAL FILES:\n#bureau_balance=r'C:\\Users\\sschm\\Desktop\\kaggle\\creditRisk\\bureau_balance.csv'\n#train=r'C:\\Users\\sschm\\Desktop\\kaggle\\creditRisk\\application_train.csv'\n#test=r'C:\\Users\\sschm\\Desktop\\kaggle\\creditRisk\\application_test.csv'\n\ndata=pd.read_csv(train) # (307511, 122)\ntestDF=pd.read_csv(test)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:39:47.198951Z","iopub.execute_input":"2022-03-26T21:39:47.199663Z","iopub.status.idle":"2022-03-26T21:39:55.673425Z","shell.execute_reply.started":"2022-03-26T21:39:47.199552Z","shell.execute_reply":"2022-03-26T21:39:55.672380Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:39:55.675128Z","iopub.execute_input":"2022-03-26T21:39:55.675425Z","iopub.status.idle":"2022-03-26T21:39:55.684801Z","shell.execute_reply.started":"2022-03-26T21:39:55.675396Z","shell.execute_reply":"2022-03-26T21:39:55.682626Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Examine TARGET column\nHow many loans were not repaid? In train.csv 0 stands for repaid and 1 stands for payment difficulties. The percent of loans that defaulted was 0.081. This is somewhat unbalanced data so we must be careful when selecting what metrics to use to analyze the data. In addition, we must consider other data files for feature importance. There are no missing TARGET values which is good because we would likely have to drop a row that does not have an independent variable.","metadata":{}},{"cell_type":"code","source":"temp=data['TARGET'].value_counts()\nprint(temp)\npaid=temp[0]\nnotPaid=temp[1]\ndefault=round(notPaid/(paid+notPaid),3)\nprint(\"Percent of loans that defauled: \", default)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:39:55.686233Z","iopub.execute_input":"2022-03-26T21:39:55.686731Z","iopub.status.idle":"2022-03-26T21:39:55.704727Z","shell.execute_reply.started":"2022-03-26T21:39:55.686683Z","shell.execute_reply":"2022-03-26T21:39:55.703646Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Numeric DataFrame\nThe data is now all numeric values. We can encode categorical values later on to view if any of the categorical values have any importance worth investing in later.","metadata":{}},{"cell_type":"code","source":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf = data.select_dtypes(include=numerics) # (307511, 106)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:39:55.707099Z","iopub.execute_input":"2022-03-26T21:39:55.707359Z","iopub.status.idle":"2022-03-26T21:39:55.818943Z","shell.execute_reply.started":"2022-03-26T21:39:55.707332Z","shell.execute_reply":"2022-03-26T21:39:55.817933Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Find missing values\nToo many missing values on a column will get the column removed. In this case, there were about forty columns with more than 50% missing data. In total, there are 60 numeric columns with missing data, we need to interpret the Bureau to find feature importance in order to engineer which columns are most worth keeping.","metadata":{}},{"cell_type":"code","source":"#search for columns with missing values:\ndef findNA():\n    print(\"Missing data by column as a percent:\")\n    findNA=df.isnull().sum().sort_values(ascending=False)/len(df)\n    print(findNA.head(40))\nfindNA() ","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:39:55.820191Z","iopub.execute_input":"2022-03-26T21:39:55.820468Z","iopub.status.idle":"2022-03-26T21:39:55.885954Z","shell.execute_reply.started":"2022-03-26T21:39:55.820437Z","shell.execute_reply":"2022-03-26T21:39:55.884918Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Fix Missing Values\nVariable number can be changed to delete a missing column with more than 20 NA values. Then, the dataframe must be filled in the mean for the remaining missing values. Since we need a final credit prediction for every credit loan, we can simply not delete any rows.","metadata":{}},{"cell_type":"code","source":"number=50 #remove col with  or more missing values\ndf = df[df.isnull().sum(axis=1) <= number] \ndf= df.fillna(df.mean())\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:39:55.887282Z","iopub.execute_input":"2022-03-26T21:39:55.887584Z","iopub.status.idle":"2022-03-26T21:39:56.470952Z","shell.execute_reply.started":"2022-03-26T21:39:55.887543Z","shell.execute_reply":"2022-03-26T21:39:56.469751Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Heat Map Correlations and Multicollinearity\nThere is no major multicollinearity. In fact, there are not many correlated variables. The following heatmap is set for correlations above .05 because there are so few variables that are highly correlated.","metadata":{}},{"cell_type":"code","source":"def printHeat():\n    corr = df.corr()\n    #print(corr)\n    y='TARGET'\n    highly_corr_features = corr.index[abs(corr[y])>0.05]\n    plt.figure(figsize=(10,10))\n    heat = sns.heatmap(df[highly_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n    top10=corr[y].sort_values(ascending=False).head(10)\n    print(heat)\n    print(\"Top 10 Correlations:\\n\", top10) # top ten correlations\nprintHeat()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:39:56.472088Z","iopub.execute_input":"2022-03-26T21:39:56.472330Z","iopub.status.idle":"2022-03-26T21:40:06.145451Z","shell.execute_reply.started":"2022-03-26T21:39:56.472301Z","shell.execute_reply":"2022-03-26T21:40:06.144607Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Split Data\nSplit the data set into training data and test data. TARGET will always be Y since it is the independent variable. A 1 is a troubled loan while a 0 equals a not distressed loan. ","metadata":{}},{"cell_type":"code","source":"X=df.drop('TARGET', axis=1)\ny=df['TARGET']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=13)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:06.146590Z","iopub.execute_input":"2022-03-26T21:40:06.146833Z","iopub.status.idle":"2022-03-26T21:40:06.557683Z","shell.execute_reply.started":"2022-03-26T21:40:06.146804Z","shell.execute_reply":"2022-03-26T21:40:06.556627Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Booster and Feature Importance\nThe amount of annuity and days_birth (age) are the two most highly correlated features. However, since this is unbalanced data there do not seem to be many feature importance to begin with.","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nfrom sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor\n\nparams = {\n \"n_estimators\": 5, \"max_depth\": 4, \"min_samples_split\": 5, \"learning_rate\": 0.01,\n}\n\n#Fit and Predict:\nreg = ensemble.GradientBoostingRegressor(**params)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\n\n#Calculate Metrics:\ngbr_r2 = r2_score(y_test, y_pred).round(4) \nprint(\"Gradient boosting regression r2: \", gbr_r2) \n\nauc = round(metrics.roc_auc_score(y_test, y_pred), 4 ) \nprint(\"AUC for gradient boost is: \", auc)\n\nmse = mean_squared_error(y_test, reg.predict(X_test))\nprint(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n\n#FEATURE IMPORTANCE:\nnum=10 # How many features?\ncols=X.columns\nfeature_importance = reg.feature_importances_[:num]\nsorted_idx = np.argsort(feature_importance)[:num]\npos = np.arange(sorted_idx.shape[0]) + 0.5\nfig = plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align=\"center\")\nplt.yticks(pos, np.array(cols)[sorted_idx])\nplt.title(\"Feature Importance (MDI)\")","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:06.559037Z","iopub.execute_input":"2022-03-26T21:40:06.559378Z","iopub.status.idle":"2022-03-26T21:40:21.852220Z","shell.execute_reply.started":"2022-03-26T21:40:06.559334Z","shell.execute_reply":"2022-03-26T21:40:21.851289Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression\nAUC for logistic regression is:  0.6145 with mse at 0.0709.\n\n The c parameter in logistic regression model by definition is the following: \"Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization\". Using 1 the default value for C or putting C at .01 did not change the AUC for the logistic regression.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(solver='liblinear') #solver param gets rid of encoder error\n\n#Train the model and create predictions\nlog_reg.fit(X_train, y_train)\n\n#use model to predict probability that given y value is 1:\nlog_reg_pred = log_reg.predict_proba(X_test)[::,1]\n\n#calculate AUC of model\nauc = round(metrics.roc_auc_score(y_test, log_reg_pred), 4 ) \nprint(\"AUC for logistic regression is: \", auc)\n\n#Mean Squared Error\nmse = mean_squared_error(y_test, log_reg_pred)\nprint(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:21.854399Z","iopub.execute_input":"2022-03-26T21:40:21.854688Z","iopub.status.idle":"2022-03-26T21:40:30.556028Z","shell.execute_reply.started":"2022-03-26T21:40:21.854658Z","shell.execute_reply":"2022-03-26T21:40:30.555000Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression\nDue to small Y indepdent variables AUC is the more accurate metric than r_squared. Since the linear regression, accuracy, and cross validate are all near .045 it seems there is no sign of overfitting.\n\nAUC for linear regression is:  0.7327 <br/>\nAccuracy:  0.0475 <br/>\n0.0489  linear regression cross validate mean <br/>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\n\n#Fit and predict:\nlrModel = LinearRegression()\nlrModel.fit(X_train, y_train)\nlrPredict = lrModel.predict(X_test)\n\n# plt.scatter(y_test, predictions)\nplt.hist(y_test - lrPredict)\n\n#Linear Metrics:\nauc = round( metrics.roc_auc_score(y_test, lrPredict), 4 ) \nr2 = r2_score(y_test, lrPredict).round(4) \nprint(\"AUC for linear regression is: \", auc)\nprint(\"Linear regression r2 score: \", r2)\n\n#CROSS VALIDATE TEST RESULTS:\nlr_score = lrModel.score(X_test, y_test).round(4)  # train test \nprint(\"Linear Accuracy: \", lr_score)\nlr_cv = cross_validate(lrModel, X, y, cv = 5, scoring= 'r2')\nlr_cvMean=lr_cv['test_score'].mean().round(4)\nprint(lr_cvMean, \" linear regression cross validate mean\")\n\ndef linearReports():\n    print(model.coef_)    \n    print(model.intercept_)\n    print(classification_report(y_test_data, lrPredict))\n    print(confusion_matrix(y_test_data, lrPredict))\n    metrics.mean_absolute_error(y_test, lrPredict)\n    \n    #Mean Sqaured Error:\n    lrMSE=np.sqrt(metrics.mean_squared_error(y_test, lrPredict))\n    print(round(lrMSE, 4), \" is lr MSE \")","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:30.562385Z","iopub.execute_input":"2022-03-26T21:40:30.563446Z","iopub.status.idle":"2022-03-26T21:40:42.295932Z","shell.execute_reply.started":"2022-03-26T21:40:30.563398Z","shell.execute_reply":"2022-03-26T21:40:42.294915Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree\nAUC for decision tree is:  0.7215 using a number for max_leaf_nodes that reduces error.","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:50:17.902367Z","iopub.execute_input":"2022-02-18T21:50:17.903218Z","iopub.status.idle":"2022-02-18T21:50:17.906851Z","shell.execute_reply.started":"2022-02-18T21:50:17.903177Z","shell.execute_reply":"2022-02-18T21:50:17.905835Z"}}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\n#FIND best_tree_size LEAF NODES:\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=42)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\ndef calcLeaf():\n    candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n    maeDic={} #dictionary  key=leaf  mae=value\n    for leaf in candidate_max_leaf_nodes:\n        mae=get_mae(leaf, X_train, X_test, y_train, y_test)\n        maeDic[leaf]=mae\n\n    best_tree_size = sorted(maeDic, key=lambda x : maeDic[x])[0]\n    print(best_tree_size, \" best_tree_size\")\n    # 500  best_tree_size\n\nbest_tree_size=500\n    \n#MAKE PREDICTION:\ntree = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=42)\ntree.fit(X, y)\ny_pred = tree.predict(X_test)\n\n#AUC and r2 metric:\ntreeR2 = r2_score(y_test, y_pred).round(4)\ntreeAUC = round( metrics.roc_auc_score(y_test, y_pred), 4 ) \nprint(\"AUC for decision tree is: \", treeAUC)\n\ndef printReports(y_test, y_pred):\n    print(classification_report(y_test, y_pred))\n    print(confusion_matrix(y_test, y_pred))\n    \n    #Mean Sqaured Error:\n    treeMSE=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n    print(round(treeMSE, 4), \" is tree MSE \")","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:42.297036Z","iopub.execute_input":"2022-03-26T21:40:42.297285Z","iopub.status.idle":"2022-03-26T21:40:52.890840Z","shell.execute_reply.started":"2022-03-26T21:40:42.297257Z","shell.execute_reply":"2022-03-26T21:40:52.889687Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Regressor\nRandom forest AUC:  0.6312. Checking for the MAE with least error has long run-time so it is important to use checkMAE as a function only when needed. Since this is unbalanced data, r_squared provides not accurate results.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n#Check for Error and find Best n_estimators:\ndef checkMAE():\n    print(\"Starting MAE:\")\n    dMAE={} #dictionary of n_estimators as key and MAE as value:\n    for n in range(2, 500, 100):\n        forest = RandomForestRegressor(n_estimators=n, random_state = 0)\n        forest.fit(X_train, y_train)\n        y_pred = forest.predict(X_test)\n        MAE=metrics.mean_absolute_error(y_test, y_pred).round(2)\n        dMAE[n]=MAE\n        print(\"n_estimates: \", n,  '  Mean Absolute Error:', MAE)\n\n    dMAE=sorted(((v, k) for k, v in dMAE.items()), reverse=False)\n    print(dMAE)\n#checkMAE() #turn function on or off by uncommenting","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:52.892631Z","iopub.execute_input":"2022-03-26T21:40:52.893417Z","iopub.status.idle":"2022-03-26T21:40:52.900700Z","shell.execute_reply.started":"2022-03-26T21:40:52.893376Z","shell.execute_reply":"2022-03-26T21:40:52.900104Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"#### Forest fit and predict","metadata":{"execution":{"iopub.status.busy":"2022-03-18T20:40:19.816119Z","iopub.execute_input":"2022-03-18T20:40:19.816471Z","iopub.status.idle":"2022-03-18T20:40:19.821056Z","shell.execute_reply.started":"2022-03-18T20:40:19.81642Z","shell.execute_reply":"2022-03-18T20:40:19.819985Z"}}},{"cell_type":"code","source":"def forest():\n    num=10\n    forest = RandomForestRegressor(n_estimators=num, random_state = 0)\n    forest.fit(X_train, y_train)\n    y_pred = forest.predict(X_test)\n\n    #Print Metrics:\n    forest_r2 = r2_score(y_test, y_pred).round(4)  \n    forest_auc = round( metrics.roc_auc_score(y_test, y_pred), 4 ) \n    print(\"Random forest AUC: \", forest_auc) \n    print(\"Random forest r2: \", forest_r2)\n\ndef forestReports():\n    mae=metrics.mean_absolute_error(y_test, y_pred).round(2)\n    print(\"Random forest MAE: \", mae)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:52.901966Z","iopub.execute_input":"2022-03-26T21:40:52.902351Z","iopub.status.idle":"2022-03-26T21:40:52.914921Z","shell.execute_reply.started":"2022-03-26T21:40:52.902309Z","shell.execute_reply":"2022-03-26T21:40:52.914189Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## K-Nearest Neighbors (KNN)\nFirst, we must select the optimal K value with the least amount of error. When graphing the error rates, 3 is the knn that provides the least amount of error. Because the large amount of data, the KNN model runs very slow which is a big issue for KNN. If you do run the KNN model, after some time, the final AUC result is around 0.5184 which was the worse predictive model. ","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ndef knnError():\n    print(\"Selecting an optimal K value:\")\n    error_rates = []\n    for i in range(1, 10, 2): #Must be an odd number to break a tie\n        new_model = KNeighborsClassifier(n_neighbors = i)\n        new_model.fit(X_train, y_train)\n        new_predictions = new_model.predict(X_test)\n        error_rates.append(np.mean(new_predictions != y_test))\n    plt.figure(figsize=(16,12))\n    plt.plot(error_rates)\n\ndef knnModel():\n    #Train the model and make predictions:\n    knn = KNeighborsClassifier(n_neighbors =3) \n    knn.fit(X_train, y_train)\n    knnPredict = knn.predict_proba(X_test)[::,1]\n\n    #calculate AUC of model\n    knn_auc = round( metrics.roc_auc_score(y_test, knnPredict), 4 ) \n    print(\"Knn AUC: \", knn_auc)\n\ndef knnReports():\n    acc = metrics.accuracy_score(y_test_data, knnPredict)\n    print(confusion_matrix(y_test, knnPredict))\n    print(classification_report(y_test, knnPredict))\n    print(confusion_matrix(y_test, knnPredict))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:52.916498Z","iopub.execute_input":"2022-03-26T21:40:52.917326Z","iopub.status.idle":"2022-03-26T21:40:52.934074Z","shell.execute_reply.started":"2022-03-26T21:40:52.917271Z","shell.execute_reply":"2022-03-26T21:40:52.933177Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Support Vector Machine (SVM)\nWith many samples, SVM is extremely slow and cannot really be used. This is a major disadvantage of SVM and why some do not use it. This seems to be the case this this problem too. For more information on why SVM is slow here is a stack overflow article: https://stackoverflow.com/questions/40077432/why-is-scikit-learn-svm-svc-extremely-slow","metadata":{}},{"cell_type":"code","source":"def trySVM():\n    from sklearn.svm import SVC\n    \n    #Fit and Predict:\n    svc = SVC()\n    svc.fit(X_train, y_train)\n    svc_predit = svc.predict(X_test)\n\n    #calculate AUC of model\n    auc = round( metrics.roc_auc_score(y_test, svc_predit), 4 ) \n    print(\"SVC AUC is: \", auc)\n\ndef svmReports():\n    print(classification_report(y_test, svc_predit))\n    print(confusion_matrix(y_test, svc_predit))\n    metrics.mean_absolute_error(y_test, svc_predit)\n    metrics.mean_squared_error(y_test, svc_predit)\n    np.sqrt(metrics.mean_squared_error(y_test, svc_predit))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:52.935788Z","iopub.execute_input":"2022-03-26T21:40:52.936757Z","iopub.status.idle":"2022-03-26T21:40:52.948707Z","shell.execute_reply.started":"2022-03-26T21:40:52.936707Z","shell.execute_reply":"2022-03-26T21:40:52.947579Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# View Test Dataset \nA final prediction needs to be made for each of the 48744 cleints. The shape of the test data is (48744, 121). ","metadata":{}},{"cell_type":"code","source":"print(testDF.shape)\ntestDF.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:52.953440Z","iopub.execute_input":"2022-03-26T21:40:52.954333Z","iopub.status.idle":"2022-03-26T21:40:52.992369Z","shell.execute_reply.started":"2022-03-26T21:40:52.954297Z","shell.execute_reply":"2022-03-26T21:40:52.991492Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineer Test Data\nTo make a final prediction, the same columns used for the train data should be used for the test data. Missing values must be filled with the mean. Dropping a column would mean a house does not get a prediction so that cannot be done.","metadata":{}},{"cell_type":"code","source":"features=list(X.columns)\ntestDF=testDF[features]\ntestDF=testDF.fillna(testDF.mean())\ntestDF.head() #5 rows Ã— 105 columns","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:52.994147Z","iopub.execute_input":"2022-03-26T21:40:52.994762Z","iopub.status.idle":"2022-03-26T21:40:53.136044Z","shell.execute_reply.started":"2022-03-26T21:40:52.994718Z","shell.execute_reply":"2022-03-26T21:40:53.134975Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Predict Final Credit Risk\nAll 307,511 credit loans from the train dataset will get a final credit risk prediction. Since linear regression seemed to have good AUC results, I will use linear for the final prediction. \n\nEach SK_ID_CURR in the test set, will predict a probability for the TARGET variable. The final prediction file should contain a header and have the following format: <br><br>\nSK_ID_CURR,TARGET <br/>\n100001, 0.1 <br/>\n100005, 0.9 <br/>\n100013, 0.2 <br/>\n","metadata":{}},{"cell_type":"code","source":"test_predictions = lrModel.predict(testDF).round(1)\ntest_predictions=np.where(test_predictions<0, 0, test_predictions)\nSK_ID_CURR=testDF['SK_ID_CURR']\ntupleData = list(zip(SK_ID_CURR, test_predictions))\noutput = pd.DataFrame(tupleData, columns = ['SK_ID_CURR', 'TARGET'])\nprint(output.shape)\noutput.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:53.137354Z","iopub.execute_input":"2022-03-26T21:40:53.137623Z","iopub.status.idle":"2022-03-26T21:40:53.446501Z","shell.execute_reply.started":"2022-03-26T21:40:53.137585Z","shell.execute_reply":"2022-03-26T21:40:53.445506Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Submit Predictions\nThe final shape is (48744, 2), the same amount of original ID on the original test data.","metadata":{}},{"cell_type":"code","source":"output.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:53.448057Z","iopub.execute_input":"2022-03-26T21:40:53.448368Z","iopub.status.idle":"2022-03-26T21:40:53.572309Z","shell.execute_reply.started":"2022-03-26T21:40:53.448327Z","shell.execute_reply":"2022-03-26T21:40:53.571379Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Extra: Bureau Data\nThe Bureau data has [1716428 rows x 17 columns]. Three columns were categorical, so they get removed. Then an additional four columns had lots of missing data, more than 80% so they are deleted. Finally, we remove a small portion of missing values just to get a general analysis of the missing data. The goal is to use this additional information outside of the train set to try to find feature importance.\n","metadata":{}},{"cell_type":"code","source":"buraeuData=r'/kaggle/input/home-credit-default-risk/bureau.csv'\nburaeuDF=pd.read_csv(buraeuData) #[1716428 rows x 17 columns]\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nburaeuDF = buraeuDF.select_dtypes(include=numerics) #(1716428, 14)\n# Six columns have missing values:\nbNA=buraeuDF.isnull().sum().sort_values(ascending=False)/len(buraeuDF) \nburaeuDF=buraeuDF.dropna(thresh=0.8*len(buraeuDF), axis=1) #(1716428, 10)\nburaeuDF = buraeuDF.dropna() #(1376391, 10)\nhead=buraeuDF.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:40:53.573877Z","iopub.execute_input":"2022-03-26T21:40:53.574572Z","iopub.status.idle":"2022-03-26T21:40:59.958692Z","shell.execute_reply.started":"2022-03-26T21:40:53.574524Z","shell.execute_reply":"2022-03-26T21:40:59.957264Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Resources\n1. https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction\n2. https://www.kaggle.com/ersinztrk/home-credit-clean-code\n3. https://www.kaggle.com/taikiikuta/home-credit-logistic-regression-modeling-at-first","metadata":{}}]}